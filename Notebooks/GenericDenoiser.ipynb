{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a48HzTgzH5df",
      "metadata": {
        "id": "a48HzTgzH5df"
      },
      "source": [
        "## 1. Download dei dati ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be9f56cb",
      "metadata": {
        "id": "be9f56cb"
      },
      "outputs": [],
      "source": [
        "# Download dataset\n",
        "!wget https://www.zemris.fer.hr/projects/LicensePlates/english/baza_slika.zip\n",
        "# Unzip file zip\n",
        "!unzip -o -j baza_slika.zip \"*.jpg\" -d dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f687007c",
      "metadata": {
        "id": "f687007c"
      },
      "source": [
        "## 2. Preparazione dei dati ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "68ce22ff",
      "metadata": {
        "id": "68ce22ff"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from PIL import Image\n",
        "from skimage.transform import rescale\n",
        "from skimage.util import random_noise\n",
        "from torch.utils.data import DataLoader, random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "87d7819b",
      "metadata": {
        "id": "87d7819b"
      },
      "outputs": [],
      "source": [
        "# Percorso alla cartella che contiene le immagini\n",
        "datasetPath = \"dataset\"\n",
        "\n",
        "# Lista per salvare le immagini caricate\n",
        "imgList = []\n",
        "\n",
        "# Scorrimento di tutti i file nella cartella\n",
        "i = 0\n",
        "for fileName in os.listdir(datasetPath):\n",
        "    if fileName.lower().endswith(\".jpg\"):\n",
        "        filePath = os.path.join(datasetPath, fileName)\n",
        "        img = np.array(Image.open(filePath)).astype(np.float32) / 255.0\n",
        "        img = rescale(img, (1/1.66, 1/1.66, 1)) # Passaggio da 640x480 a 386x289\n",
        "        imgList.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6a34481a",
      "metadata": {
        "id": "6a34481a"
      },
      "outputs": [],
      "source": [
        "# Creazione rumore\n",
        "def addNoise(img, intensity=0.2):\n",
        "    # Scelgo casualmente se applicare un rumore sale e pepe o uno gaussiano\n",
        "    randInt = random.randint(1,4) \n",
        "    h,w,c = img.shape\n",
        "    y = np.zeros_like(img)\n",
        "    if randInt == 1:                    # Rumore gaussiano\n",
        "        d = random.randint(10, 25)/255\n",
        "        n = d*np.random.randn(h,w,c)\n",
        "        y = img+n\n",
        "    elif randInt == 2:                  # Rumore sale e pepe\n",
        "        sp = random_noise(img, mode=\"s&p\")\n",
        "        y = img*sp\n",
        "    elif randInt == 3:                  # Rumore poisson\n",
        "        poi = random_noise(img, mode=\"poisson\")\n",
        "        y = img*poi\n",
        "    else:                               # Rumore speckle\n",
        "        n = np.random.randn(h,w,c) \n",
        "        y = img + img*n*intensity            \n",
        "    \n",
        "    if y.max() > 1.0:\n",
        "        y[y > 1.0] = 1.0\n",
        "    if y.min() < 0.0:\n",
        "        y[y < 0.0] = 0.0\n",
        "    \n",
        "    return y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bd875f2",
      "metadata": {
        "id": "0bd875f2"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "\n",
        "# Modifica del dataset\n",
        "class ImgListDataset(Dataset):\n",
        "    def __init__(self, imgList):\n",
        "        self.imgList = imgList\n",
        "        self.target_size = (289, 386)           # Altezza e larghezza desiderata\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),            # Conversione a immagine\n",
        "            transforms.Resize(self.target_size),\n",
        "            transforms.ToTensor()               # Conversione a tensore\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgList)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.imgList[idx]\n",
        "        if isinstance(img, np.ndarray):\n",
        "            img = self.transform(img)\n",
        "        return img\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22cf6d4e",
      "metadata": {
        "id": "22cf6d4e"
      },
      "outputs": [],
      "source": [
        "# Divisione dell'intero dataset in training set, validation set e test set (80%, 10% e 10%)\n",
        "trainSet, valSet, testSet = random_split(imgList, [0.8, 0.1, 0.1])\n",
        "\n",
        "# Creazione dei dataset di immagini rumorose\n",
        "trainSetNoise = []\n",
        "valSetNoise = []\n",
        "testSetNoise = []\n",
        "\n",
        "for el in trainSet:\n",
        "    imgNoise = addNoise(el)\n",
        "    trainSetNoise.append(imgNoise)\n",
        "\n",
        "for el in valSet:\n",
        "    imgNoise = addNoise(el)\n",
        "    valSetNoise.append(imgNoise)\n",
        "\n",
        "for el in testSet:\n",
        "    imgNoise = addNoise(el)\n",
        "    testSetNoise.append(imgNoise)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2af2e0e1",
      "metadata": {
        "id": "2af2e0e1",
        "outputId": "2980e7fd-3485-4ce0-8b9f-13fe4b5d246c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Effettuata suddivisione:\n",
            "- Training-set: 403 campioni.\n",
            "- Validation-set: 50 campioni.\n",
            "- Test-set: 50 campioni.\n",
            "- Training-set Noise: 403 campioni.\n",
            "- Validation-set Noise: 50 campioni.\n",
            "- Test-set Noise: 50 campioni.\n"
          ]
        }
      ],
      "source": [
        "# Definizione variabili\n",
        "batchSize = 6\n",
        "numWorkers = 4\n",
        "\n",
        "# Modifica del dataset per le immagini originali\n",
        "trainSetDataset = ImgListDataset(trainSet)\n",
        "valSetDataset = ImgListDataset(valSet)\n",
        "testSetDataset = ImgListDataset(testSet)\n",
        "\n",
        "# Dataloader per le immagini originali\n",
        "trainDataload = DataLoader(trainSetDataset, batch_size=batchSize, num_workers=numWorkers)\n",
        "valDataload = DataLoader(valSetDataset, batch_size=batchSize, num_workers=numWorkers)\n",
        "testDataload = DataLoader(testSetDataset, batch_size=batchSize, num_workers=numWorkers)\n",
        "\n",
        "# Modifica del dataset per le immagini rumorose\n",
        "trainSetRicDataset = ImgListDataset(trainSetNoise)\n",
        "valSetRicDataset = ImgListDataset(valSetNoise)\n",
        "testSetRicDataset = ImgListDataset(testSetNoise)\n",
        "\n",
        "# Dataloader per le immagini rumorose\n",
        "trainDataloadNoise = DataLoader(trainSetRicDataset, batch_size=batchSize, num_workers=numWorkers)\n",
        "valDataloadNoise = DataLoader(valSetRicDataset, batch_size=batchSize, num_workers=numWorkers)\n",
        "testDataloadNoise = DataLoader(testSetRicDataset, batch_size=batchSize, num_workers=numWorkers)\n",
        "\n",
        "print(\"Effettuata suddivisione:\")\n",
        "print(f\"- Training-set: {len(trainSet)} campioni.\")\n",
        "print(f\"- Validation-set: {len(valSet)} campioni.\")\n",
        "print(f\"- Test-set: {len(testSet)} campioni.\")\n",
        "print(f\"- Training-set Noise: {len(trainSetNoise)} campioni.\")\n",
        "print(f\"- Validation-set Noise: {len(valSetNoise)} campioni.\")\n",
        "print(f\"- Test-set Noise: {len(testSetNoise)} campioni.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b895ccf1",
      "metadata": {
        "id": "b895ccf1"
      },
      "source": [
        "## 3. Architettura ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "440f2cea",
      "metadata": {
        "id": "440f2cea"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DnCNN(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=64, kernel_size=3, padding=1):\n",
        "        super(DnCNN, self).__init__()\n",
        "        # in_channel = 3 poichÃ¨ 3 canali\n",
        "        # out_channel = 64 come scritto nella traccia (num feat)\n",
        "        # kernel_size = 3 come scritto nella traccia (dim spaziale 3x3)\n",
        "        layers = []\n",
        "\n",
        "        # Iter 1: Convolution + ReLU\n",
        "        layers.append(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, padding=padding))\n",
        "        layers.append(nn.ReLU())\n",
        "\n",
        "        # Iters 2-19: Convolution + BatchNorm + ReLU\n",
        "        for i in range(18):\n",
        "            layers.append(nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size, padding=padding))\n",
        "            layers.append(nn.BatchNorm2d(out_channels))                    # Corrisponde al numero di canali di output del livello precedente\n",
        "            layers.append(nn.ReLU())\n",
        "\n",
        "        # Iters 20: Convolution\n",
        "        layers.append(nn.Conv2d(in_channels=out_channels, out_channels=in_channels, kernel_size=kernel_size, padding=padding))\n",
        "\n",
        "        # *layers corrisponde a layers[0],layers[1],..., quindi giÃ  spalmati come singoli elementi\n",
        "        self.features = nn.Sequential(*layers)\n",
        "\n",
        "        self.apply(self.kernel_initializer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.features(x)\n",
        "\n",
        "    # Equivalente a kernel_inizializer=\"Orthogonal\" in pytorch\n",
        "    def kernel_initializer(self, module):\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            nn.init.orthogonal_(module.weight)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76be591b",
      "metadata": {
        "id": "76be591b"
      },
      "source": [
        "## 4. Addestramento ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d154678e",
      "metadata": {
        "id": "d154678e"
      },
      "outputs": [],
      "source": [
        "# Importiamo le librerie necessarie\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52c4f5aa",
      "metadata": {
        "id": "52c4f5aa",
        "outputId": "c71eaa96-2f86-43aa-917a-fa1a8e56e753"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Abilitazione del dispositivo GPU per il training\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "770ddf07",
      "metadata": {
        "id": "770ddf07"
      },
      "outputs": [],
      "source": [
        "# Legenda dataloader:\n",
        "#   - dlTrain = immagini originali training\n",
        "#   - dlTrainNoise = immagini rumorose training\n",
        "#   - dlVal = immagini originali validation\n",
        "#   - dlValNoise = immagini rumorose validation\n",
        "def training (dlTrain, dlTrainNoise, dlVal, dlValNoise, numEpoch, model, criterion, optimizer, bestMse, bestPsnr, outputPath):\n",
        "\n",
        "    # Liste dei risultati definite per la visualizzazione\n",
        "    MSETrainList = []\n",
        "    MSEValList = []\n",
        "    PSNRTrainList = []\n",
        "    PSNRValList = []\n",
        "\n",
        "    # Iterazione per ogni epoch\n",
        "    for epoch in range(numEpoch):\n",
        "\n",
        "        model.cuda()\n",
        "        # Conteggio del tempo per misurare la durata di un'epoca\n",
        "        since = time.time()\n",
        "\n",
        "        # Inizializzazione delle variabili\n",
        "        modelMseTrain = 0.0\n",
        "        totalSize = 0\n",
        "\n",
        "        # Modello impostato in traning mode\n",
        "        model.train()\n",
        "\n",
        "        # Iterazione per ogni batch\n",
        "        # Devo iterare su i 2 dataloader contemporaneamente, per fare ciÃ² utilizzo zip\n",
        "        for (inputsTrain, inputsNoise) in zip(dlTrain, dlTrainNoise):\n",
        "\n",
        "            # Converto gli input in tensori float e li carico nella GPU\n",
        "            inputsTrain = inputsTrain.type(torch.FloatTensor).cuda()\n",
        "            inputsNoise = inputsNoise.type(torch.FloatTensor).cuda()\n",
        "\n",
        "            # Reset dei gradienti, altrimenti i vecchi gradienti sono sommati ai nuovi, piuttosto\n",
        "            # che essere sovrascritti\n",
        "            optimizer.zero_grad()\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Calcolo dettagli\n",
        "            yTrain = model(inputsNoise)\n",
        "            y = inputsNoise - yTrain\n",
        "\n",
        "            # Calcolo della MSE\n",
        "            loss = criterion(y, inputsTrain)\n",
        "            # size(0) restituisce il numero di campioni nel batch, quindi si sta moltiplicando la loss\n",
        "            # media per il numero di elementi per ottenere la somma totale della loss\n",
        "            modelMseTrain += loss.item() * inputsTrain.size(0)\n",
        "            totalSize += inputsTrain.size(0)\n",
        "\n",
        "            # Calcolando il gradiente del tensore attuale\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()        # Aggiornamento dei parametri\n",
        "            optimizer.zero_grad()   # Azzeramento dei gradienti per il prossimo ciclo di accumulo\n",
        "            \n",
        "\n",
        "        # Calcolo della MSE medio e del PSNR medio  dell'epoch\n",
        "        modelMseEpochTrain = modelMseTrain/totalSize\n",
        "        modelPsnrEpochTrain = 10 * torch.log10(torch.tensor(1/modelMseEpochTrain))\n",
        "\n",
        "        # Salvataggio dei pesi per ogni iterazione (disabilitato)\n",
        "        # torch.save(model.state_dict(), outputPath + \"train_weights.pth\")\n",
        "\n",
        "        # Modello impostato in validation mode\n",
        "        model.eval()\n",
        "\n",
        "        # Inizializzazione delle variabili\n",
        "        modelMseVal = 0.0\n",
        "        totalSizeVal = 0\n",
        "\n",
        "        with torch.no_grad():               # Disattiva il calcolo dei gradienti\n",
        "        # Iterazione per ogni bach\n",
        "        # Devo iterare su i 2 dataloader contemporaneamente, per fare ciÃ² utilizzo zip\n",
        "            for (inputsVal, inputsValNoise) in zip(dlVal, dlValNoise):\n",
        "\n",
        "                # Converto gli input in tensori float e li carico nella GPU\n",
        "                inputsVal = inputsVal.type(torch.FloatTensor).cuda()\n",
        "                inputsValNoise = inputsValNoise.type(torch.FloatTensor).cuda()\n",
        "\n",
        "                # Calcolo dettagli\n",
        "                yVal = model(inputsValNoise)\n",
        "                y = inputsValNoise - yVal \n",
        "\n",
        "                # Calcolo della MSE\n",
        "                loss = criterion(y, inputsVal)\n",
        "                # size(0) restituisce il numero di campioni nel batch, quindi si sta moltiplicando la loss\n",
        "                # media per il numero di elementi per ottenere la somma totale della loss\n",
        "                modelMseVal += loss.item() * inputsVal.size(0)\n",
        "                totalSizeVal += inputsVal.size(0)\n",
        "\n",
        "            # Calcolo della MSE medio e del PSNR medio  dell'epoch\n",
        "            modelMseEpochVal = modelMseVal/totalSizeVal\n",
        "            modelPsnrEpochVal = 10 * torch.log10(torch.tensor(1/modelMseEpochVal))\n",
        "            timeElapsed = time.time()-since\n",
        "\n",
        "        print('[Epoch %d][Train on %d [MSE: %.4f  PSNR: %.4f]][Val on %d [MSE: %.4f  PSNR: %.4f]][Time: %.0f m %.0f s]'\n",
        "                %(epoch, totalSize, modelMseEpochTrain, modelPsnrEpochTrain, totalSizeVal, modelMseEpochVal,\n",
        "                modelPsnrEpochVal, timeElapsed // 60, timeElapsed % 60))\n",
        "\n",
        "        # Salvaggio dei risultati migliori\n",
        "        if (modelMseEpochVal < bestMse):\n",
        "            print(\"-------Saving best weights-------\")\n",
        "            bestMse = modelMseEpochVal\n",
        "            bestPsnr = modelPsnrEpochVal\n",
        "            # Salvataggio dei migliori risultati\n",
        "            try:\n",
        "                torch.save(model.cpu().state_dict(), outputPath)\n",
        "                print(\"-------Best weights saved-------\")\n",
        "            except Exception as e:\n",
        "                print(\"Error:\", e)\n",
        "\n",
        "        # Salvataggio dei risultati per la visualizzazione\n",
        "        MSETrainList.append(modelMseEpochTrain)\n",
        "        MSEValList.append(modelMseEpochVal)\n",
        "        PSNRTrainList.append(modelPsnrEpochTrain)\n",
        "        PSNRValList.append(modelPsnrEpochVal)\n",
        "\n",
        "    return bestMse, bestPsnr, MSETrainList, MSEValList, PSNRTrainList, PSNRValList\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c13e3c40",
      "metadata": {
        "id": "c13e3c40"
      },
      "outputs": [],
      "source": [
        "# Funzione di testing\n",
        "\n",
        "def testing (dlTest, dlTestNoise, model, criterion, weightPath):\n",
        "\n",
        "    # Caricamento dei pesi\n",
        "    model.load_state_dict(torch.load(weightPath, map_location=torch.device(\"cpu\")))\n",
        "    model.cuda()\n",
        "\n",
        "\n",
        "    # Conteggio del tempo per misurare la durata di un'epoca\n",
        "    since = time.time()\n",
        "\n",
        "    # Inizializzazione delle variabili\n",
        "    modelMseTest = 0.0\n",
        "    totalSize = 0\n",
        "    outputsTest = []\n",
        "\n",
        "\n",
        "    #model.load_state_dict(torch.load(weightPath , map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    # Iterazione su batch\n",
        "    with torch.no_grad():               # Disattiva il calcolo dei gradienti\n",
        "        for (inputsTest, inputsTestNoise) in zip(dlTest, dlTestNoise):\n",
        "\n",
        "\n",
        "            # Converto gli input in tensori float e li carico nella GPU\n",
        "            inputsTest = inputsTest.type(torch.FloatTensor).cuda()\n",
        "            inputsTestNoise = inputsTestNoise.type(torch.FloatTensor).cuda()\n",
        "\n",
        "            # Calcolo dettagli e ricostruzione immagine\n",
        "            yTest = model(inputsTestNoise)\n",
        "            y = inputsTestNoise - yTest\n",
        "            outputsTest.append(y)\n",
        "\n",
        "            # Calcolo della MSE\n",
        "            loss = criterion(y, inputsTest)\n",
        "            # size(0) restituisce il numero di campioni nel batch, quindi si sta moltiplicando la loss\n",
        "            # media per il numero di elementi per ottenere la somma totale della loss\n",
        "            modelMseTest += loss.item() * inputsTest.size(0)\n",
        "            totalSize += inputsTest.size(0)\n",
        "\n",
        "\n",
        "        # Calcolo della MSE medio e del PSNR medio  dell'epoch\n",
        "        modelMseEpochTest = modelMseTest/totalSize\n",
        "        modelPsnrEpochTest = 10 * torch.log10(torch.tensor(1/modelMseEpochTest))\n",
        "        timeElapsed = time.time()-since\n",
        "\n",
        "        print(\"[Test] [MSE: %.4f  PSNR: %.4f] [Time: %.0f m %.0f s]\"\n",
        "          %(modelMseEpochTest, modelPsnrEpochTest, timeElapsed // 60, timeElapsed % 60))\n",
        "\n",
        "        return outputsTest"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca6fc029",
      "metadata": {
        "id": "ca6fc029"
      },
      "source": [
        "## 5. Valutazione delle prestazioni ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b5cd243",
      "metadata": {
        "id": "6b5cd243"
      },
      "outputs": [],
      "source": [
        "# Iper-parametri\n",
        "epochList = [60]\n",
        "learningRateList = [0.01, 0.001, 0.0001]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25666195",
      "metadata": {
        "id": "25666195"
      },
      "outputs": [],
      "source": [
        "# Main\n",
        "import torch.optim as optim\n",
        "\n",
        "model = DnCNN().cuda()\n",
        "criterion = torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')\n",
        "\n",
        "bestMSEComb = 9000000000000000000.0              # Miglior MSE tra tutte le combinazioni\n",
        "bestPSNRComb = -900000000000000000.0             # Miglior PSNR tra tutte le combinazioni\n",
        "\n",
        "# Definizione dei percorsi per il salvataggio dei pesi\n",
        "weightPath = \"best_weights_generic_denoise.pth\"\n",
        "weightPathComb = \"best_weights_generic_denoise_lr.pth\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dfc263c",
      "metadata": {
        "id": "8dfc263c",
        "outputId": "3e36426b-4069-4919-d0a2-e62a2930b6c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 0][Train on 403 [MSE: 0.3734  PSNR: 4.2778]][Val on 50 [MSE: 0.0154  PSNR: 18.1290]][Time: 0 m 52 s]\n",
            "-------Saving best weights-------\n",
            "-------Best weights saved-------\n",
            "[Epoch 1][Train on 403 [MSE: 0.0139  PSNR: 18.5660]][Val on 50 [MSE: 0.0142  PSNR: 18.4654]][Time: 0 m 52 s]\n",
            "-------Saving best weights-------\n",
            "-------Best weights saved-------\n",
            "[Epoch 2][Train on 403 [MSE: 0.0150  PSNR: 18.2367]][Val on 50 [MSE: 0.0131  PSNR: 18.8296]][Time: 0 m 52 s]\n",
            "-------Saving best weights-------\n",
            "-------Best weights saved-------\n",
            "[Epoch 3][Train on 403 [MSE: 0.0144  PSNR: 18.4305]][Val on 50 [MSE: 0.0128  PSNR: 18.9377]][Time: 0 m 52 s]\n",
            "-------Saving best weights-------\n",
            "-------Best weights saved-------\n",
            "[Epoch 4][Train on 403 [MSE: 0.0137  PSNR: 18.6314]][Val on 50 [MSE: 0.0128  PSNR: 18.9361]][Time: 0 m 52 s]\n",
            "[Epoch 5][Train on 403 [MSE: 0.0134  PSNR: 18.7253]][Val on 50 [MSE: 0.0144  PSNR: 18.4167]][Time: 0 m 52 s]\n",
            "[Epoch 6][Train on 403 [MSE: 0.0133  PSNR: 18.7711]][Val on 50 [MSE: 0.0134  PSNR: 18.7273]][Time: 0 m 52 s]\n",
            "[Epoch 7][Train on 403 [MSE: 0.0131  PSNR: 18.8315]][Val on 50 [MSE: 0.0135  PSNR: 18.6808]][Time: 0 m 52 s]\n",
            "[Epoch 8][Train on 403 [MSE: 0.0131  PSNR: 18.8301]][Val on 50 [MSE: 0.0135  PSNR: 18.7046]][Time: 0 m 52 s]\n",
            "[Epoch 9][Train on 403 [MSE: 0.0131  PSNR: 18.8407]][Val on 50 [MSE: 0.0136  PSNR: 18.6604]][Time: 0 m 52 s]\n",
            "[Epoch 10][Train on 403 [MSE: 0.0130  PSNR: 18.8574]][Val on 50 [MSE: 0.0136  PSNR: 18.6645]][Time: 0 m 52 s]\n",
            "[Epoch 11][Train on 403 [MSE: 0.0129  PSNR: 18.8811]][Val on 50 [MSE: 0.0137  PSNR: 18.6228]][Time: 0 m 52 s]\n",
            "[Epoch 12][Train on 403 [MSE: 0.0129  PSNR: 18.8944]][Val on 50 [MSE: 0.0136  PSNR: 18.6589]][Time: 0 m 52 s]\n",
            "[Epoch 13][Train on 403 [MSE: 0.0129  PSNR: 18.8922]][Val on 50 [MSE: 0.0139  PSNR: 18.5832]][Time: 0 m 52 s]\n",
            "[Epoch 14][Train on 403 [MSE: 0.0130  PSNR: 18.8662]][Val on 50 [MSE: 0.0139  PSNR: 18.5826]][Time: 0 m 52 s]\n",
            "[Epoch 15][Train on 403 [MSE: 0.0129  PSNR: 18.8981]][Val on 50 [MSE: 0.0138  PSNR: 18.6056]][Time: 0 m 52 s]\n",
            "[Epoch 16][Train on 403 [MSE: 0.0128  PSNR: 18.9124]][Val on 50 [MSE: 0.0141  PSNR: 18.5118]][Time: 0 m 52 s]\n",
            "[Epoch 17][Train on 403 [MSE: 0.0128  PSNR: 18.9169]][Val on 50 [MSE: 0.0149  PSNR: 18.2555]][Time: 0 m 52 s]\n",
            "[Epoch 18][Train on 403 [MSE: 0.0128  PSNR: 18.9274]][Val on 50 [MSE: 0.0141  PSNR: 18.4993]][Time: 0 m 52 s]\n",
            "[Epoch 19][Train on 403 [MSE: 0.0129  PSNR: 18.8917]][Val on 50 [MSE: 0.0138  PSNR: 18.5899]][Time: 0 m 52 s]\n",
            "[Epoch 20][Train on 403 [MSE: 0.0128  PSNR: 18.9173]][Val on 50 [MSE: 0.0142  PSNR: 18.4671]][Time: 0 m 52 s]\n",
            "[Epoch 21][Train on 403 [MSE: 0.0129  PSNR: 18.8894]][Val on 50 [MSE: 0.0150  PSNR: 18.2294]][Time: 0 m 52 s]\n",
            "[Epoch 22][Train on 403 [MSE: 0.0128  PSNR: 18.9321]][Val on 50 [MSE: 0.0141  PSNR: 18.5136]][Time: 0 m 52 s]\n",
            "[Epoch 23][Train on 403 [MSE: 0.0128  PSNR: 18.9325]][Val on 50 [MSE: 0.0141  PSNR: 18.5060]][Time: 0 m 52 s]\n",
            "[Epoch 24][Train on 403 [MSE: 0.0127  PSNR: 18.9471]][Val on 50 [MSE: 0.0142  PSNR: 18.4793]][Time: 0 m 52 s]\n",
            "[Epoch 25][Train on 403 [MSE: 0.0127  PSNR: 18.9471]][Val on 50 [MSE: 0.0154  PSNR: 18.1266]][Time: 0 m 52 s]\n",
            "[Epoch 26][Train on 403 [MSE: 0.0135  PSNR: 18.7111]][Val on 50 [MSE: 0.0142  PSNR: 18.4872]][Time: 0 m 52 s]\n",
            "[Epoch 27][Train on 403 [MSE: 0.0132  PSNR: 18.7873]][Val on 50 [MSE: 0.0142  PSNR: 18.4727]][Time: 0 m 52 s]\n",
            "[Epoch 28][Train on 403 [MSE: 0.0136  PSNR: 18.6750]][Val on 50 [MSE: 0.0143  PSNR: 18.4463]][Time: 0 m 52 s]\n",
            "[Epoch 29][Train on 403 [MSE: 0.0130  PSNR: 18.8632]][Val on 50 [MSE: 0.0142  PSNR: 18.4881]][Time: 0 m 52 s]\n",
            "[Epoch 30][Train on 403 [MSE: 0.0129  PSNR: 18.8834]][Val on 50 [MSE: 0.0136  PSNR: 18.6777]][Time: 0 m 52 s]\n",
            "[Epoch 31][Train on 403 [MSE: 0.0129  PSNR: 18.8970]][Val on 50 [MSE: 0.0140  PSNR: 18.5309]][Time: 0 m 52 s]\n",
            "[Epoch 32][Train on 403 [MSE: 0.0128  PSNR: 18.9207]][Val on 50 [MSE: 0.0133  PSNR: 18.7475]][Time: 0 m 52 s]\n",
            "[Epoch 33][Train on 403 [MSE: 0.0127  PSNR: 18.9490]][Val on 50 [MSE: 0.0146  PSNR: 18.3650]][Time: 0 m 52 s]\n",
            "[Epoch 34][Train on 403 [MSE: 0.0136  PSNR: 18.6673]][Val on 50 [MSE: 0.0148  PSNR: 18.3066]][Time: 0 m 52 s]\n",
            "[Epoch 35][Train on 403 [MSE: 0.0136  PSNR: 18.6540]][Val on 50 [MSE: 0.0145  PSNR: 18.3977]][Time: 0 m 52 s]\n",
            "[Epoch 36][Train on 403 [MSE: 0.0135  PSNR: 18.6836]][Val on 50 [MSE: 0.0144  PSNR: 18.4071]][Time: 0 m 52 s]\n",
            "[Epoch 37][Train on 403 [MSE: 0.0135  PSNR: 18.6881]][Val on 50 [MSE: 0.0145  PSNR: 18.3960]][Time: 0 m 52 s]\n",
            "[Epoch 38][Train on 403 [MSE: 0.0135  PSNR: 18.6884]][Val on 50 [MSE: 0.0145  PSNR: 18.3906]][Time: 0 m 52 s]\n",
            "[Epoch 39][Train on 403 [MSE: 0.0135  PSNR: 18.6935]][Val on 50 [MSE: 0.0144  PSNR: 18.4286]][Time: 0 m 52 s]\n",
            "[Epoch 40][Train on 403 [MSE: 0.0134  PSNR: 18.7410]][Val on 50 [MSE: 0.0146  PSNR: 18.3583]][Time: 0 m 52 s]\n",
            "[Epoch 41][Train on 403 [MSE: 0.0131  PSNR: 18.8186]][Val on 50 [MSE: 0.0150  PSNR: 18.2364]][Time: 0 m 52 s]\n",
            "[Epoch 42][Train on 403 [MSE: 0.0131  PSNR: 18.8189]][Val on 50 [MSE: 0.0151  PSNR: 18.2009]][Time: 0 m 52 s]\n",
            "[Epoch 43][Train on 403 [MSE: 0.0131  PSNR: 18.8354]][Val on 50 [MSE: 0.0142  PSNR: 18.4711]][Time: 0 m 52 s]\n",
            "[Epoch 44][Train on 403 [MSE: 0.0130  PSNR: 18.8475]][Val on 50 [MSE: 0.0152  PSNR: 18.1745]][Time: 0 m 52 s]\n",
            "[Epoch 45][Train on 403 [MSE: 0.0131  PSNR: 18.8377]][Val on 50 [MSE: 0.0148  PSNR: 18.2858]][Time: 0 m 52 s]\n",
            "[Epoch 46][Train on 403 [MSE: 0.0130  PSNR: 18.8529]][Val on 50 [MSE: 0.0143  PSNR: 18.4365]][Time: 0 m 52 s]\n",
            "[Epoch 47][Train on 403 [MSE: 0.0130  PSNR: 18.8457]][Val on 50 [MSE: 0.0149  PSNR: 18.2626]][Time: 0 m 52 s]\n",
            "[Epoch 48][Train on 403 [MSE: 0.0130  PSNR: 18.8599]][Val on 50 [MSE: 0.0142  PSNR: 18.4746]][Time: 0 m 52 s]\n",
            "[Epoch 49][Train on 403 [MSE: 0.0130  PSNR: 18.8495]][Val on 50 [MSE: 0.0147  PSNR: 18.3360]][Time: 0 m 52 s]\n",
            "[Epoch 50][Train on 403 [MSE: 0.0130  PSNR: 18.8590]][Val on 50 [MSE: 0.0146  PSNR: 18.3538]][Time: 0 m 52 s]\n",
            "[Epoch 51][Train on 403 [MSE: 0.0130  PSNR: 18.8530]][Val on 50 [MSE: 0.0147  PSNR: 18.3328]][Time: 0 m 52 s]\n",
            "[Epoch 52][Train on 403 [MSE: 0.0130  PSNR: 18.8640]][Val on 50 [MSE: 0.0147  PSNR: 18.3242]][Time: 0 m 52 s]\n",
            "[Epoch 53][Train on 403 [MSE: 0.0130  PSNR: 18.8527]][Val on 50 [MSE: 0.0156  PSNR: 18.0712]][Time: 0 m 52 s]\n",
            "[Epoch 54][Train on 403 [MSE: 0.0130  PSNR: 18.8626]][Val on 50 [MSE: 0.0147  PSNR: 18.3367]][Time: 0 m 52 s]\n",
            "[Epoch 55][Train on 403 [MSE: 0.0130  PSNR: 18.8653]][Val on 50 [MSE: 0.0155  PSNR: 18.0866]][Time: 0 m 52 s]\n",
            "[Epoch 56][Train on 403 [MSE: 0.0130  PSNR: 18.8586]][Val on 50 [MSE: 0.0148  PSNR: 18.3083]][Time: 0 m 52 s]\n",
            "[Epoch 57][Train on 403 [MSE: 0.0129  PSNR: 18.8789]][Val on 50 [MSE: 0.0140  PSNR: 18.5479]][Time: 0 m 52 s]\n",
            "[Epoch 58][Train on 403 [MSE: 0.0130  PSNR: 18.8636]][Val on 50 [MSE: 0.0151  PSNR: 18.2209]][Time: 0 m 52 s]\n",
            "[Epoch 59][Train on 403 [MSE: 0.0129  PSNR: 18.8833]][Val on 50 [MSE: 0.0155  PSNR: 18.0963]][Time: 0 m 52 s]\n",
            "------Best Combination saved [Epoch: 60 - Learning Rate: 0.01]-------\n",
            "[Epoch 0][Train on 403 [MSE: 0.0126  PSNR: 18.9888]][Val on 50 [MSE: 0.0130  PSNR: 18.8722]][Time: 0 m 52 s]\n",
            "-------Saving best weights-------\n",
            "-------Best weights saved-------\n",
            "[Epoch 1][Train on 403 [MSE: 0.0124  PSNR: 19.0703]][Val on 50 [MSE: 0.0129  PSNR: 18.8962]][Time: 0 m 52 s]\n",
            "-------Saving best weights-------\n",
            "-------Best weights saved-------\n",
            "[Epoch 2][Train on 403 [MSE: 0.0123  PSNR: 19.0840]][Val on 50 [MSE: 0.0121  PSNR: 19.1880]][Time: 0 m 52 s]\n",
            "-------Saving best weights-------\n",
            "-------Best weights saved-------\n",
            "[Epoch 3][Train on 403 [MSE: 0.0124  PSNR: 19.0827]][Val on 50 [MSE: 0.0127  PSNR: 18.9660]][Time: 0 m 52 s]\n",
            "[Epoch 4][Train on 403 [MSE: 0.0122  PSNR: 19.1352]][Val on 50 [MSE: 0.0129  PSNR: 18.9035]][Time: 0 m 52 s]\n",
            "[Epoch 5][Train on 403 [MSE: 0.0122  PSNR: 19.1257]][Val on 50 [MSE: 0.0125  PSNR: 19.0146]][Time: 0 m 52 s]\n",
            "[Epoch 6][Train on 403 [MSE: 0.0121  PSNR: 19.1850]][Val on 50 [MSE: 0.0120  PSNR: 19.2101]][Time: 0 m 52 s]\n",
            "-------Saving best weights-------\n",
            "-------Best weights saved-------\n",
            "[Epoch 7][Train on 403 [MSE: 0.0119  PSNR: 19.2404]][Val on 50 [MSE: 0.0117  PSNR: 19.3332]][Time: 0 m 52 s]\n",
            "-------Saving best weights-------\n",
            "-------Best weights saved-------\n",
            "[Epoch 8][Train on 403 [MSE: 0.0118  PSNR: 19.2776]][Val on 50 [MSE: 0.0129  PSNR: 18.9067]][Time: 0 m 52 s]\n",
            "[Epoch 9][Train on 403 [MSE: 0.0117  PSNR: 19.3077]][Val on 50 [MSE: 0.0113  PSNR: 19.4677]][Time: 0 m 52 s]\n",
            "-------Saving best weights-------\n",
            "-------Best weights saved-------\n",
            "[Epoch 10][Train on 403 [MSE: 0.0116  PSNR: 19.3529]][Val on 50 [MSE: 0.0115  PSNR: 19.3824]][Time: 0 m 52 s]\n",
            "[Epoch 11][Train on 403 [MSE: 0.0112  PSNR: 19.5024]][Val on 50 [MSE: 0.0120  PSNR: 19.2112]][Time: 0 m 52 s]\n",
            "[Epoch 12][Train on 403 [MSE: 0.0110  PSNR: 19.5837]][Val on 50 [MSE: 0.0110  PSNR: 19.6049]][Time: 0 m 52 s]\n",
            "-------Saving best weights-------\n",
            "-------Best weights saved-------\n",
            "[Epoch 13][Train on 403 [MSE: 0.0110  PSNR: 19.5884]][Val on 50 [MSE: 0.0118  PSNR: 19.2914]][Time: 0 m 52 s]\n",
            "[Epoch 14][Train on 403 [MSE: 0.0109  PSNR: 19.6237]][Val on 50 [MSE: 0.0108  PSNR: 19.6584]][Time: 0 m 53 s]\n",
            "-------Saving best weights-------\n",
            "-------Best weights saved-------\n",
            "[Epoch 15][Train on 403 [MSE: 0.0106  PSNR: 19.7419]][Val on 50 [MSE: 0.0134  PSNR: 18.7308]][Time: 0 m 53 s]\n",
            "[Epoch 16][Train on 403 [MSE: 0.0104  PSNR: 19.8122]][Val on 50 [MSE: 0.0104  PSNR: 19.8261]][Time: 0 m 53 s]\n",
            "-------Saving best weights-------\n",
            "-------Best weights saved-------\n",
            "[Epoch 17][Train on 403 [MSE: 0.0103  PSNR: 19.8757]][Val on 50 [MSE: 0.0118  PSNR: 19.2701]][Time: 0 m 52 s]\n",
            "[Epoch 18][Train on 403 [MSE: 0.0100  PSNR: 20.0026]][Val on 50 [MSE: 0.0105  PSNR: 19.7971]][Time: 0 m 52 s]\n",
            "[Epoch 19][Train on 403 [MSE: 0.0100  PSNR: 19.9808]][Val on 50 [MSE: 0.0111  PSNR: 19.5602]][Time: 0 m 52 s]\n",
            "[Epoch 20][Train on 403 [MSE: 0.0098  PSNR: 20.0952]][Val on 50 [MSE: 0.0114  PSNR: 19.4324]][Time: 0 m 52 s]\n",
            "[Epoch 21][Train on 403 [MSE: 0.0096  PSNR: 20.1840]][Val on 50 [MSE: 0.0107  PSNR: 19.7079]][Time: 0 m 52 s]\n",
            "[Epoch 22][Train on 403 [MSE: 0.0096  PSNR: 20.1904]][Val on 50 [MSE: 0.0115  PSNR: 19.4029]][Time: 0 m 52 s]\n",
            "[Epoch 23][Train on 403 [MSE: 0.0093  PSNR: 20.3218]][Val on 50 [MSE: 0.0107  PSNR: 19.6862]][Time: 0 m 52 s]\n",
            "[Epoch 24][Train on 403 [MSE: 0.0090  PSNR: 20.4359]][Val on 50 [MSE: 0.0099  PSNR: 20.0566]][Time: 0 m 52 s]\n",
            "-------Saving best weights-------\n",
            "-------Best weights saved-------\n",
            "[Epoch 25][Train on 403 [MSE: 0.0090  PSNR: 20.4627]][Val on 50 [MSE: 0.0099  PSNR: 20.0472]][Time: 0 m 53 s]\n",
            "[Epoch 26][Train on 403 [MSE: 0.0091  PSNR: 20.4295]][Val on 50 [MSE: 0.0092  PSNR: 20.3832]][Time: 0 m 52 s]\n",
            "-------Saving best weights-------\n",
            "-------Best weights saved-------\n",
            "[Epoch 27][Train on 403 [MSE: 0.0087  PSNR: 20.5929]][Val on 50 [MSE: 0.0092  PSNR: 20.3448]][Time: 0 m 53 s]\n",
            "[Epoch 28][Train on 403 [MSE: 0.0087  PSNR: 20.5922]][Val on 50 [MSE: 0.0094  PSNR: 20.2528]][Time: 0 m 53 s]\n",
            "[Epoch 29][Train on 403 [MSE: 0.0085  PSNR: 20.7314]][Val on 50 [MSE: 0.0092  PSNR: 20.3511]][Time: 0 m 52 s]\n",
            "[Epoch 30][Train on 403 [MSE: 0.0084  PSNR: 20.7444]][Val on 50 [MSE: 0.0086  PSNR: 20.6698]][Time: 0 m 52 s]\n",
            "-------Saving best weights-------\n",
            "-------Best weights saved-------\n",
            "[Epoch 31][Train on 403 [MSE: 0.0082  PSNR: 20.8416]][Val on 50 [MSE: 0.0099  PSNR: 20.0233]][Time: 0 m 53 s]\n",
            "[Epoch 32][Train on 403 [MSE: 0.0083  PSNR: 20.8223]][Val on 50 [MSE: 0.0084  PSNR: 20.7317]][Time: 0 m 53 s]\n",
            "-------Saving best weights-------\n",
            "-------Best weights saved-------\n",
            "[Epoch 33][Train on 403 [MSE: 0.0081  PSNR: 20.9281]][Val on 50 [MSE: 0.0084  PSNR: 20.7327]][Time: 0 m 52 s]\n",
            "-------Saving best weights-------\n",
            "-------Best weights saved-------\n",
            "[Epoch 34][Train on 403 [MSE: 0.0080  PSNR: 20.9807]][Val on 50 [MSE: 0.0081  PSNR: 20.9359]][Time: 0 m 53 s]\n",
            "-------Saving best weights-------\n",
            "-------Best weights saved-------\n",
            "[Epoch 35][Train on 403 [MSE: 0.0080  PSNR: 20.9484]][Val on 50 [MSE: 0.0102  PSNR: 19.9307]][Time: 0 m 52 s]\n",
            "[Epoch 36][Train on 403 [MSE: 0.0080  PSNR: 20.9843]][Val on 50 [MSE: 0.0075  PSNR: 21.2388]][Time: 0 m 52 s]\n",
            "-------Saving best weights-------\n",
            "-------Best weights saved-------\n",
            "[Epoch 37][Train on 403 [MSE: 0.0077  PSNR: 21.1528]][Val on 50 [MSE: 0.0076  PSNR: 21.1678]][Time: 0 m 52 s]\n",
            "[Epoch 38][Train on 403 [MSE: 0.0076  PSNR: 21.1830]][Val on 50 [MSE: 0.0072  PSNR: 21.4404]][Time: 0 m 52 s]\n",
            "-------Saving best weights-------\n",
            "-------Best weights saved-------\n",
            "[Epoch 39][Train on 403 [MSE: 0.0074  PSNR: 21.2990]][Val on 50 [MSE: 0.0073  PSNR: 21.3617]][Time: 0 m 53 s]\n",
            "[Epoch 40][Train on 403 [MSE: 0.0073  PSNR: 21.3923]][Val on 50 [MSE: 0.0068  PSNR: 21.6513]][Time: 0 m 52 s]\n",
            "-------Saving best weights-------\n",
            "-------Best weights saved-------\n",
            "[Epoch 41][Train on 403 [MSE: 0.0072  PSNR: 21.4316]][Val on 50 [MSE: 0.0069  PSNR: 21.6262]][Time: 0 m 53 s]\n",
            "[Epoch 42][Train on 403 [MSE: 0.0071  PSNR: 21.5132]][Val on 50 [MSE: 0.0067  PSNR: 21.7356]][Time: 0 m 53 s]\n",
            "-------Saving best weights-------\n",
            "-------Best weights saved-------\n",
            "[Epoch 43][Train on 403 [MSE: 0.0069  PSNR: 21.6348]][Val on 50 [MSE: 0.0068  PSNR: 21.6820]][Time: 0 m 52 s]\n",
            "[Epoch 44][Train on 403 [MSE: 0.0068  PSNR: 21.6534]][Val on 50 [MSE: 0.0066  PSNR: 21.8212]][Time: 0 m 52 s]\n",
            "-------Saving best weights-------\n",
            "-------Best weights saved-------\n",
            "[Epoch 45][Train on 403 [MSE: 0.0068  PSNR: 21.7008]][Val on 50 [MSE: 0.0065  PSNR: 21.8602]][Time: 0 m 52 s]\n",
            "-------Saving best weights-------\n",
            "-------Best weights saved-------\n",
            "[Epoch 46][Train on 403 [MSE: 0.0068  PSNR: 21.6566]][Val on 50 [MSE: 0.0072  PSNR: 21.4220]][Time: 0 m 53 s]\n",
            "[Epoch 47][Train on 403 [MSE: 0.0066  PSNR: 21.8153]][Val on 50 [MSE: 0.0070  PSNR: 21.5458]][Time: 0 m 52 s]\n",
            "[Epoch 48][Train on 403 [MSE: 0.0065  PSNR: 21.8725]][Val on 50 [MSE: 0.0068  PSNR: 21.6991]][Time: 0 m 53 s]\n",
            "[Epoch 49][Train on 403 [MSE: 0.0064  PSNR: 21.9342]][Val on 50 [MSE: 0.0075  PSNR: 21.2707]][Time: 0 m 53 s]\n",
            "[Epoch 50][Train on 403 [MSE: 0.0063  PSNR: 21.9809]][Val on 50 [MSE: 0.0085  PSNR: 20.7152]][Time: 0 m 53 s]\n",
            "[Epoch 51][Train on 403 [MSE: 0.0063  PSNR: 21.9922]][Val on 50 [MSE: 0.0066  PSNR: 21.7890]][Time: 0 m 53 s]\n",
            "[Epoch 52][Train on 403 [MSE: 0.0062  PSNR: 22.0434]][Val on 50 [MSE: 0.0064  PSNR: 21.9296]][Time: 0 m 53 s]\n",
            "-------Saving best weights-------\n",
            "-------Best weights saved-------\n",
            "[Epoch 53][Train on 403 [MSE: 0.0061  PSNR: 22.1366]][Val on 50 [MSE: 0.0057  PSNR: 22.4597]][Time: 0 m 53 s]\n",
            "-------Saving best weights-------\n",
            "-------Best weights saved-------\n",
            "[Epoch 54][Train on 403 [MSE: 0.0060  PSNR: 22.2457]][Val on 50 [MSE: 0.0061  PSNR: 22.1305]][Time: 0 m 53 s]\n",
            "[Epoch 55][Train on 403 [MSE: 0.0060  PSNR: 22.2302]][Val on 50 [MSE: 0.0069  PSNR: 21.5970]][Time: 0 m 52 s]\n",
            "[Epoch 56][Train on 403 [MSE: 0.0061  PSNR: 22.1400]][Val on 50 [MSE: 0.0068  PSNR: 21.6801]][Time: 0 m 53 s]\n",
            "[Epoch 57][Train on 403 [MSE: 0.0061  PSNR: 22.1608]][Val on 50 [MSE: 0.0056  PSNR: 22.5186]][Time: 0 m 52 s]\n",
            "-------Saving best weights-------\n",
            "-------Best weights saved-------\n",
            "[Epoch 58][Train on 403 [MSE: 0.0062  PSNR: 22.0422]][Val on 50 [MSE: 0.0067  PSNR: 21.7104]][Time: 0 m 52 s]\n",
            "[Epoch 59][Train on 403 [MSE: 0.0064  PSNR: 21.9163]][Val on 50 [MSE: 0.0063  PSNR: 22.0202]][Time: 0 m 53 s]\n",
            "------Best Combination saved [Epoch: 60 - Learning Rate: 0.001]-------\n",
            "[Epoch 0][Train on 403 [MSE: 0.0056  PSNR: 22.4821]][Val on 50 [MSE: 0.0058  PSNR: 22.3687]][Time: 0 m 52 s]\n",
            "-------Saving best weights-------\n",
            "-------Best weights saved-------\n",
            "[Epoch 1][Train on 403 [MSE: 0.0054  PSNR: 22.6967]][Val on 50 [MSE: 0.0057  PSNR: 22.4391]][Time: 0 m 52 s]\n",
            "-------Saving best weights-------\n",
            "-------Best weights saved-------\n",
            "[Epoch 2][Train on 403 [MSE: 0.0053  PSNR: 22.7583]][Val on 50 [MSE: 0.0057  PSNR: 22.4690]][Time: 0 m 52 s]\n",
            "-------Saving best weights-------\n",
            "-------Best weights saved-------\n",
            "[Epoch 3][Train on 403 [MSE: 0.0052  PSNR: 22.8087]][Val on 50 [MSE: 0.0056  PSNR: 22.4893]][Time: 0 m 52 s]\n",
            "-------Saving best weights-------\n",
            "-------Best weights saved-------\n",
            "[Epoch 4][Train on 403 [MSE: 0.0052  PSNR: 22.8511]][Val on 50 [MSE: 0.0056  PSNR: 22.5024]][Time: 0 m 52 s]\n",
            "-------Saving best weights-------\n",
            "-------Best weights saved-------\n",
            "[Epoch 5][Train on 403 [MSE: 0.0051  PSNR: 22.8892]][Val on 50 [MSE: 0.0056  PSNR: 22.5093]][Time: 0 m 52 s]\n",
            "-------Saving best weights-------\n",
            "-------Best weights saved-------\n",
            "[Epoch 6][Train on 403 [MSE: 0.0051  PSNR: 22.9226]][Val on 50 [MSE: 0.0056  PSNR: 22.5117]][Time: 0 m 52 s]\n",
            "-------Saving best weights-------\n",
            "-------Best weights saved-------\n",
            "[Epoch 7][Train on 403 [MSE: 0.0051  PSNR: 22.9533]][Val on 50 [MSE: 0.0056  PSNR: 22.5107]][Time: 0 m 53 s]\n",
            "[Epoch 8][Train on 403 [MSE: 0.0050  PSNR: 22.9815]][Val on 50 [MSE: 0.0056  PSNR: 22.5045]][Time: 0 m 53 s]\n",
            "[Epoch 9][Train on 403 [MSE: 0.0050  PSNR: 23.0077]][Val on 50 [MSE: 0.0056  PSNR: 22.4952]][Time: 0 m 53 s]\n",
            "[Epoch 10][Train on 403 [MSE: 0.0050  PSNR: 23.0318]][Val on 50 [MSE: 0.0056  PSNR: 22.4827]][Time: 0 m 53 s]\n",
            "[Epoch 11][Train on 403 [MSE: 0.0050  PSNR: 23.0537]][Val on 50 [MSE: 0.0057  PSNR: 22.4710]][Time: 0 m 53 s]\n",
            "[Epoch 12][Train on 403 [MSE: 0.0049  PSNR: 23.0739]][Val on 50 [MSE: 0.0057  PSNR: 22.4635]][Time: 0 m 53 s]\n",
            "[Epoch 13][Train on 403 [MSE: 0.0049  PSNR: 23.0924]][Val on 50 [MSE: 0.0057  PSNR: 22.4581]][Time: 0 m 53 s]\n",
            "[Epoch 14][Train on 403 [MSE: 0.0049  PSNR: 23.1096]][Val on 50 [MSE: 0.0057  PSNR: 22.4540]][Time: 0 m 53 s]\n",
            "[Epoch 15][Train on 403 [MSE: 0.0049  PSNR: 23.1258]][Val on 50 [MSE: 0.0057  PSNR: 22.4514]][Time: 0 m 53 s]\n",
            "[Epoch 16][Train on 403 [MSE: 0.0049  PSNR: 23.1410]][Val on 50 [MSE: 0.0057  PSNR: 22.4488]][Time: 0 m 52 s]\n",
            "[Epoch 17][Train on 403 [MSE: 0.0048  PSNR: 23.1553]][Val on 50 [MSE: 0.0057  PSNR: 22.4457]][Time: 0 m 53 s]\n",
            "[Epoch 18][Train on 403 [MSE: 0.0048  PSNR: 23.1691]][Val on 50 [MSE: 0.0057  PSNR: 22.4427]][Time: 0 m 53 s]\n",
            "[Epoch 19][Train on 403 [MSE: 0.0048  PSNR: 23.1823]][Val on 50 [MSE: 0.0057  PSNR: 22.4396]][Time: 0 m 52 s]\n",
            "[Epoch 20][Train on 403 [MSE: 0.0048  PSNR: 23.1949]][Val on 50 [MSE: 0.0057  PSNR: 22.4385]][Time: 0 m 53 s]\n",
            "[Epoch 21][Train on 403 [MSE: 0.0048  PSNR: 23.2070]][Val on 50 [MSE: 0.0057  PSNR: 22.4367]][Time: 0 m 52 s]\n",
            "[Epoch 22][Train on 403 [MSE: 0.0048  PSNR: 23.2185]][Val on 50 [MSE: 0.0057  PSNR: 22.4349]][Time: 0 m 53 s]\n",
            "[Epoch 23][Train on 403 [MSE: 0.0048  PSNR: 23.2295]][Val on 50 [MSE: 0.0057  PSNR: 22.4328]][Time: 0 m 53 s]\n",
            "[Epoch 24][Train on 403 [MSE: 0.0047  PSNR: 23.2403]][Val on 50 [MSE: 0.0057  PSNR: 22.4334]][Time: 0 m 52 s]\n",
            "[Epoch 25][Train on 403 [MSE: 0.0047  PSNR: 23.2508]][Val on 50 [MSE: 0.0057  PSNR: 22.4348]][Time: 0 m 52 s]\n",
            "[Epoch 26][Train on 403 [MSE: 0.0047  PSNR: 23.2611]][Val on 50 [MSE: 0.0057  PSNR: 22.4347]][Time: 0 m 52 s]\n",
            "[Epoch 27][Train on 403 [MSE: 0.0047  PSNR: 23.2710]][Val on 50 [MSE: 0.0057  PSNR: 22.4363]][Time: 0 m 52 s]\n",
            "[Epoch 28][Train on 403 [MSE: 0.0047  PSNR: 23.2807]][Val on 50 [MSE: 0.0057  PSNR: 22.4379]][Time: 0 m 52 s]\n",
            "[Epoch 29][Train on 403 [MSE: 0.0047  PSNR: 23.2902]][Val on 50 [MSE: 0.0057  PSNR: 22.4388]][Time: 0 m 52 s]\n",
            "[Epoch 30][Train on 403 [MSE: 0.0047  PSNR: 23.2996]][Val on 50 [MSE: 0.0057  PSNR: 22.4392]][Time: 0 m 53 s]\n",
            "[Epoch 31][Train on 403 [MSE: 0.0047  PSNR: 23.3088]][Val on 50 [MSE: 0.0057  PSNR: 22.4408]][Time: 0 m 53 s]\n",
            "[Epoch 32][Train on 403 [MSE: 0.0047  PSNR: 23.3181]][Val on 50 [MSE: 0.0057  PSNR: 22.4443]][Time: 0 m 53 s]\n",
            "[Epoch 33][Train on 403 [MSE: 0.0046  PSNR: 23.3271]][Val on 50 [MSE: 0.0057  PSNR: 22.4460]][Time: 0 m 53 s]\n",
            "[Epoch 34][Train on 403 [MSE: 0.0046  PSNR: 23.3361]][Val on 50 [MSE: 0.0057  PSNR: 22.4504]][Time: 0 m 52 s]\n",
            "[Epoch 35][Train on 403 [MSE: 0.0046  PSNR: 23.3448]][Val on 50 [MSE: 0.0057  PSNR: 22.4547]][Time: 0 m 52 s]\n",
            "[Epoch 36][Train on 403 [MSE: 0.0046  PSNR: 23.3535]][Val on 50 [MSE: 0.0057  PSNR: 22.4599]][Time: 0 m 53 s]\n",
            "[Epoch 37][Train on 403 [MSE: 0.0046  PSNR: 23.3621]][Val on 50 [MSE: 0.0057  PSNR: 22.4654]][Time: 0 m 52 s]\n",
            "[Epoch 38][Train on 403 [MSE: 0.0046  PSNR: 23.3705]][Val on 50 [MSE: 0.0057  PSNR: 22.4706]][Time: 0 m 53 s]\n",
            "[Epoch 39][Train on 403 [MSE: 0.0046  PSNR: 23.3788]][Val on 50 [MSE: 0.0057  PSNR: 22.4717]][Time: 0 m 52 s]\n",
            "[Epoch 40][Train on 403 [MSE: 0.0046  PSNR: 23.3871]][Val on 50 [MSE: 0.0057  PSNR: 22.4757]][Time: 0 m 52 s]\n",
            "[Epoch 41][Train on 403 [MSE: 0.0046  PSNR: 23.3953]][Val on 50 [MSE: 0.0057  PSNR: 22.4782]][Time: 0 m 53 s]\n",
            "[Epoch 42][Train on 403 [MSE: 0.0046  PSNR: 23.4034]][Val on 50 [MSE: 0.0056  PSNR: 22.4840]][Time: 0 m 52 s]\n",
            "[Epoch 43][Train on 403 [MSE: 0.0046  PSNR: 23.4115]][Val on 50 [MSE: 0.0056  PSNR: 22.4836]][Time: 0 m 53 s]\n",
            "[Epoch 44][Train on 403 [MSE: 0.0046  PSNR: 23.4196]][Val on 50 [MSE: 0.0056  PSNR: 22.4855]][Time: 0 m 53 s]\n",
            "[Epoch 45][Train on 403 [MSE: 0.0045  PSNR: 23.4277]][Val on 50 [MSE: 0.0056  PSNR: 22.4872]][Time: 0 m 52 s]\n",
            "[Epoch 46][Train on 403 [MSE: 0.0045  PSNR: 23.4354]][Val on 50 [MSE: 0.0056  PSNR: 22.4888]][Time: 0 m 52 s]\n",
            "[Epoch 47][Train on 403 [MSE: 0.0045  PSNR: 23.4435]][Val on 50 [MSE: 0.0056  PSNR: 22.4909]][Time: 0 m 53 s]\n",
            "[Epoch 48][Train on 403 [MSE: 0.0045  PSNR: 23.4523]][Val on 50 [MSE: 0.0056  PSNR: 22.4940]][Time: 0 m 52 s]\n",
            "[Epoch 49][Train on 403 [MSE: 0.0045  PSNR: 23.4603]][Val on 50 [MSE: 0.0056  PSNR: 22.5018]][Time: 0 m 52 s]\n",
            "[Epoch 50][Train on 403 [MSE: 0.0045  PSNR: 23.4680]][Val on 50 [MSE: 0.0056  PSNR: 22.5058]][Time: 0 m 52 s]\n",
            "[Epoch 51][Train on 403 [MSE: 0.0045  PSNR: 23.4754]][Val on 50 [MSE: 0.0056  PSNR: 22.5090]][Time: 0 m 52 s]\n",
            "[Epoch 52][Train on 403 [MSE: 0.0045  PSNR: 23.4828]][Val on 50 [MSE: 0.0056  PSNR: 22.5105]][Time: 0 m 52 s]\n",
            "[Epoch 53][Train on 403 [MSE: 0.0045  PSNR: 23.4899]][Val on 50 [MSE: 0.0056  PSNR: 22.5099]][Time: 0 m 52 s]\n",
            "[Epoch 54][Train on 403 [MSE: 0.0045  PSNR: 23.4970]][Val on 50 [MSE: 0.0056  PSNR: 22.5096]][Time: 0 m 52 s]\n",
            "[Epoch 55][Train on 403 [MSE: 0.0045  PSNR: 23.5040]][Val on 50 [MSE: 0.0056  PSNR: 22.5092]][Time: 0 m 52 s]\n",
            "[Epoch 56][Train on 403 [MSE: 0.0045  PSNR: 23.5108]][Val on 50 [MSE: 0.0056  PSNR: 22.5092]][Time: 0 m 52 s]\n",
            "[Epoch 57][Train on 403 [MSE: 0.0044  PSNR: 23.5176]][Val on 50 [MSE: 0.0056  PSNR: 22.5092]][Time: 0 m 52 s]\n",
            "[Epoch 58][Train on 403 [MSE: 0.0044  PSNR: 23.5243]][Val on 50 [MSE: 0.0056  PSNR: 22.5099]][Time: 0 m 53 s]\n",
            "[Epoch 59][Train on 403 [MSE: 0.0044  PSNR: 23.5308]][Val on 50 [MSE: 0.0056  PSNR: 22.5102]][Time: 0 m 52 s]\n",
            "Best MSE: 0.005599342342466116\n",
            "Best PSNR: 22.518630981445312\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "for numEpoch in epochList:\n",
        "    for lr in learningRateList:\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "        bestMse = 9000000000000000000.0             # Miglior MSE della singola combinazione\n",
        "        bestPsnr = -900000000000000000.0            # Miglior PSNR della singola combinazione\n",
        "\n",
        "        bestMSES, bestPSNRS, MSETrainList, MSEValList, PSNRTrainList, PSNRValList = training (trainDataload, trainDataloadNoise, valDataload, valDataloadNoise, numEpoch, model, criterion, optimizer, bestMse, bestPsnr, weightPath)\n",
        "\n",
        "        # Aggiorno i valori di miglior MSE\n",
        "        if bestMSES < bestMSEComb:\n",
        "            bestMSEComb = bestMSES\n",
        "            bestPSNRComb = bestPSNRS\n",
        "            # Caricamento dei pesi\n",
        "            torch.save(model.cpu().state_dict(), weightPathComb)\n",
        "            print(f\"------Best Combination saved [Epoch: {numEpoch} - Learning Rate: {lr}]-------\")\n",
        "            model.cuda()\n",
        "\n",
        "print(f\"Best MSE: {bestMSEComb}\")\n",
        "print(f\"Best PSNR: {bestPSNRComb}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dc8e01c",
      "metadata": {
        "id": "3dc8e01c",
        "outputId": "2f946f8f-6a6a-47d0-853b-4650670a4c73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Test] [MSE: 0.0068  PSNR: 21.6957] [Time: 0 m 3 s]\n"
          ]
        }
      ],
      "source": [
        "# Percorsi output\n",
        "outputInPath = \"outputs/GenericDenoiser/testSet/\"\n",
        "outputTruthPath = \"outputs/GenericDenoiser/groundTruth/\"\n",
        "outputOutPath = \"outputs/GenericDenoiser/results/\"\n",
        "os.makedirs(outputInPath, exist_ok=True)\n",
        "os.makedirs(outputTruthPath, exist_ok=True)\n",
        "os.makedirs(outputOutPath, exist_ok=True)\n",
        "\n",
        "# Testing\n",
        "outputsTest = testing(testDataload, testDataloadNoise, model, criterion, weightPathComb)\n",
        "\n",
        "# Iterazione sia su outputsTest che su testDataloadRic\n",
        "index = 1\n",
        "testIter = iter(testDataloadNoise)\n",
        "truthIter = iter(testDataload)\n",
        "\n",
        "# Salvataggio locale delle immagini \n",
        "for batchOut in outputsTest:\n",
        "    batchIn = next(testIter)                # Ottenimento del batch originale (e label se presente)\n",
        "    batchTruth = next(truthIter)\n",
        "\n",
        "    for j in range(batchOut.size(0)):\n",
        "        # --- Output ---\n",
        "        imgOut = batchOut[j].detach().cpu().clamp(0, 1)\n",
        "        imgOut = imgOut.permute(1, 2, 0).numpy()\n",
        "        pathOut = os.path.join(outputOutPath, f\"image_{index}.jpg\")\n",
        "        plt.imsave(pathOut, imgOut)\n",
        "        # --- Input ---\n",
        "        imgIn = batchIn[j].detach().cpu().clamp(0, 1)\n",
        "        imgIn = imgIn.permute(1, 2, 0).numpy()\n",
        "        pathIn = os.path.join(outputInPath, f\"image_{index}.jpg\")\n",
        "        plt.imsave(pathIn, imgIn)\n",
        "        # --- Ground Truth ---\n",
        "        imgTruth = batchTruth[j].detach().cpu().clamp(0, 1)\n",
        "        imgTruth = imgTruth.permute(1, 2, 0).numpy()\n",
        "        pathTruth = os.path.join(outputTruthPath, f\"image_{index}.jpg\")\n",
        "        plt.imsave(pathTruth, imgTruth)\n",
        "        index += 1"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
